import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
%matplotlib inline
import numpy as np # linear algebra
import seaborn as sns #statistical data visualization
sns.set(style='whitegrid')
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

Iris=pd.read_csv("C:/Users/Lenovo/Desktop/ml/iris.csv")
Iris.head()

Iris.shape
Iris=Iris.iloc[:100,:]
Iris.shape
Iris.replace({'setosa':0,'versicolor':1},inplace=True)
Iris.head()

sns.scatterplot(x='sepal_length',y='sepal_width',hue='species',data=Iris)
plt.xlabel('SepalLength')
plt.ylabel('SepalWidth')
plt.legend(['Iris-setosa','Iris-versicolor'])
plt.show()

X = Iris.drop(labels=['species'], axis=1).values #feature matrix
Y = Iris.species.values #output

seed=5
import random
random.seed(seed)
np.random.seed(seed)
tf.set_random_seed(seed)
train_index=np.random.choice(len(X),80,replace=False)
test_index = np.array(list(set(range(len(X))) - set(train_index)))

train_X = X[train_index]
train_y = Y[train_index]
test_X = X[test_index]
test_y = Y[test_index]

def min_max_normalized(data):
  normal_data=(data-data.min())/(data.max()-data.min())
  return normal_data
X=min_max_normalized(X)

w=tf.Variable(tf.random.normal(shape=[4,1],seed=5))
b = tf.Variable(tf.random.normal(shape=[1, 1],seed=5)) #bias
#constructor for random initialization
init=tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

data = tf.placeholder(dtype=tf.float32, shape=[None, 4])#comment on the difference between tf.placeholder and tf.variable-----
#placeholder is used to input external data and it's size is variable.
#Variable is used to store values that change in the program and it's size is fixed and must be specified before
target = tf.placeholder(dtype=tf.float32, shape=[None, 1])

mod=tf.add(b,(tf.matmul(data,w))) #model

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=mod, labels=target))

#learning_rate
learning_rate=0.001

#batch_size---- #using batch makes the training ------? #faster and less accurate
batch_size=30

#iter_num
iter_num=1500

opt=tf.train.GradientDescentOptimizer(learning_rate)
# Define the goal
goal = opt.minimize(loss)


# Define the accuracy
# The default threshold is 0.5, rounded off directly
prediction = tf.round(tf.sigmoid(mod))
# Bool into float32 type
correct = tf.cast(tf.equal(prediction, target), dtype=tf.float32)
# Average
accuracy = tf.reduce_mean(correct)
# End of the definition of the model framework

# Define the variable that stores the result
loss_trace = []
train_acc = []



# training model using mini-batch gradient descent
for epoch in range(iter_num):
    # Generate random batch_index
    #randomnly picks up up 30 sample data from the train_X
    batch_index=np.random.choice(len(train_X),batch_size,replace=False)
    batch_train_X = train_X[batch_index]
    batch_train_y = np.matrix(train_y[batch_index]).T
    sess.run(goal, feed_dict={data: batch_train_X, target: batch_train_y})
    temp_loss = sess.run(loss, feed_dict={data: batch_train_X, target: batch_train_y})
    #Similarly store train accuracy value for current epoch using sess.run
    temp_train_acc=sess.run(accuracy, feed_dict={data: batch_train_X, target: batch_train_y})
    
    # storing values
    loss_trace.append(temp_loss)
    train_acc.append(temp_train_acc)
    # output
    if (epoch + 1) % 100 == 0:
        
        print('epoch: {:4d} loss: {:5f} train_acc: {:5f} '.format(epoch + 1, temp_loss,temp_train_acc))
#Write the command to calculate test accuracy:
temp_test_acc=sess.run(accuracy,feed_dict={data:test_X,target:np.matrix(test_y).T})
print("The test accuracy is: {:5f}".format(temp_test_acc))

# loss function
plt.plot(list(range(iter_num)),loss_trace)
plt.title("Cross Entropy Loss")
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

plt.plot(list(range(iter_num)),train_acc)
plt.title('Train Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()
